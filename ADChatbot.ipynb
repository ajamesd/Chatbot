{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "import collections\n",
    "from collections import Counter\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch import Tensor\n",
    "from typing import List\n",
    "import json\n",
    "import sklearn\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-13T07:47:48.816399Z",
     "end_time": "2023-05-13T07:47:51.518373Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-05-13T07:47:51.522883Z",
     "end_time": "2023-05-13T07:47:52.166047Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Load SQuAD data\n",
    "with open(\"squad2.json\", 'r') as file:\n",
    "    squad_data = json.load(file)\n",
    "\n",
    "# Extract dialogues\n",
    "dialogues = []\n",
    "for data in squad_data['data']:\n",
    "    for paragraph in data['paragraphs']:\n",
    "        for qas in paragraph['qas']:\n",
    "            if not qas['is_impossible']:\n",
    "                dialogues.append((qas['question'], qas['answers'][0]['text']))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class WordEmbeddings(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.embedding(input)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-13T07:47:52.169040Z",
     "end_time": "2023-05-13T07:47:52.175224Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch import Tensor\n",
    "from typing import List\n",
    "\n",
    "# Build vocab\n",
    "def build_vocab(sentences: List[List[str]], min_freq: int = 3):\n",
    "    counter = Counter([word for sentence in sentences for word in sentence])\n",
    "    vocab = {word: i+4 for i, (word, freq) in enumerate(counter.items()) if freq >= min_freq}\n",
    "    vocab['<PAD>'] = 0  # Padding\n",
    "    vocab['<SOS>'] = 1  # Start of sentence\n",
    "    vocab['<EOS>'] = 2  # End of sentence\n",
    "    vocab['<UNK>'] = 3  # Unknown\n",
    "    return vocab\n",
    "\n",
    "# Build reverse vocab\n",
    "def build_reverse_vocab(vocab):\n",
    "    return {i: word for word, i in vocab.items()}\n",
    "\n",
    "# Tokenize and convert to integer sequences\n",
    "def preprocess_data(data: List[str], vocab):\n",
    "    sequences = [[vocab.get(word, vocab['<UNK>']) for word in sentence.split()] for sentence in data]\n",
    "    return sequences\n",
    "\n",
    "# Convert to tensor and pad\n",
    "def pad_and_convert_to_tensor(sequences: List[List[int]]):\n",
    "    max_length = max(len(seq) for seq in sequences)\n",
    "    padded_sequences = [sequence + [0] * (max_length - len(sequence)) for sequence in sequences]\n",
    "    return pad_sequence([Tensor(sequence) for sequence in padded_sequences], batch_first=True, padding_value=0)\n",
    "\n",
    "def pad_and_convert_to_tensor(sequences: List[List[int]]):\n",
    "    max_length = max(len(seq) for seq in sequences)\n",
    "    padded_sequences = [sequence + [0] * (max_length - len(sequence)) for sequence in sequences]\n",
    "    return pad_sequence([Tensor(sequence) for sequence in padded_sequences], batch_first=True, padding_value=0)\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, question_tensors, answer_tensors):\n",
    "        self.question_tensors = question_tensors\n",
    "        self.answer_tensors = answer_tensors\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.question_tensors[index], self.answer_tensors[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.question_tensors)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Extract questions and answers\n",
    "questions, answers = zip(*dialogues)\n",
    "\n",
    "# Build vocabularies\n",
    "question_vocab = build_vocab(questions)\n",
    "answer_vocab = build_vocab(answers)\n",
    "reverse_answer_vocab = build_reverse_vocab(answer_vocab)\n",
    "\n",
    "# Preprocess questions and answers\n",
    "question_sequences = preprocess_data(questions, question_vocab)\n",
    "answer_sequences = preprocess_data(answers, answer_vocab)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-13T07:47:52.175224Z",
     "end_time": "2023-05-13T07:47:53.012801Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Convert to tensors and pad\n",
    "question_tensors = pad_and_convert_to_tensor(question_sequences)\n",
    "answer_tensors = pad_and_convert_to_tensor(answer_sequences)\n",
    "\n",
    "# Define the proportions\n",
    "train_ratio = 0.70\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "# Make sure the proportions sum to 1\n",
    "assert train_ratio + val_ratio + test_ratio == 1, \"Ratios must sum to 1\"\n",
    "\n",
    "# Calculate the sizes\n",
    "total_size = len(question_tensors)\n",
    "train_size = int(total_size * train_ratio)\n",
    "val_size = int(total_size * val_ratio)\n",
    "test_size = total_size - train_size - val_size\n",
    "\n",
    "# Split the dataset\n",
    "train_question_tensors = question_tensors[:train_size]\n",
    "val_question_tensors = question_tensors[train_size:train_size+val_size]\n",
    "test_question_tensors = question_tensors[train_size+val_size:]\n",
    "\n",
    "train_answer_tensors = answer_tensors[:train_size]\n",
    "val_answer_tensors = answer_tensors[train_size:train_size+val_size]\n",
    "test_answer_tensors = answer_tensors[train_size+val_size:]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = QADataset(train_question_tensors, train_answer_tensors)\n",
    "val_dataset = QADataset(val_question_tensors, val_answer_tensors)\n",
    "test_dataset = QADataset(test_question_tensors, test_answer_tensors)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 128  # You can change this value\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-13T07:47:53.035837Z",
     "end_time": "2023-05-13T07:47:54.415997Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Question Sequences:\n",
      "tensor([[3., 3., 3.,  ..., 0., 0., 0.],\n",
      "        [3., 3., 3.,  ..., 0., 0., 0.],\n",
      "        [3., 3., 3.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [3., 3., 3.,  ..., 0., 0., 0.],\n",
      "        [3., 3., 3.,  ..., 0., 0., 0.],\n",
      "        [3., 3., 3.,  ..., 0., 0., 0.]])\n",
      "\n",
      "Sample Answer Sequences:\n",
      "tensor([[3., 3., 3.,  ..., 0., 0., 0.],\n",
      "        [3., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [3., 3., 3.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [3., 3., 0.,  ..., 0., 0., 0.],\n",
      "        [3., 3., 3.,  ..., 0., 0., 0.],\n",
      "        [3., 3., 3.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# Extract a batch from the DataLoader\n",
    "sample_batch = next(iter(train_loader))\n",
    "\n",
    "# Print the integer sequences\n",
    "print(\"Sample Question Sequences:\")\n",
    "print(sample_batch[0])\n",
    "\n",
    "print(\"\\nSample Answer Sequences:\")\n",
    "print(sample_batch[1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-13T07:47:54.419006Z",
     "end_time": "2023-05-13T07:47:54.426808Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = src.long()  #\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        return hidden, cell\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-13T07:48:35.614863Z",
     "end_time": "2023-05-13T07:48:35.627319Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
    "\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        input = input.long()  # Convert the input tensor to torch.long datatype\n",
    "        input = input.unsqueeze(0)\n",
    "\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "\n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "\n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "\n",
    "        return prediction, hidden, cell\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-13T07:48:36.412923Z",
     "end_time": "2023-05-13T07:48:36.456711Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "\n",
    "        hidden, cell = self.encoder(src)\n",
    "\n",
    "        input = trg[0, :]\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, cell = self.decoder(input, hidden[:, :hidden.shape[1]], cell[:, :cell.shape[1]])\n",
    "\n",
    "            outputs[t] = output\n",
    "\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "\n",
    "            top1 = output.argmax(1)\n",
    "\n",
    "            input = trg[t] if teacher_force else top1\n",
    "\n",
    "        return outputs\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-13T07:48:37.349550Z",
     "end_time": "2023-05-13T07:48:37.395040Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Model:\n",
      "Encoder(\n",
      "  (embedding): Embedding(134, 256)\n",
      "  (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "\n",
      "Decoder Model:\n",
      "Decoder(\n",
      "  (embedding): Embedding(192, 256)\n",
      "  (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
      "  (fc_out): Linear(in_features=512, out_features=192, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "\n",
      "Seq2Seq Model:\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(134, 256)\n",
      "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embedding): Embedding(192, 256)\n",
      "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
      "    (fc_out): Linear(in_features=512, out_features=192, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model, optimizer and loss function\n",
    "INPUT_DIM = len(question_vocab)\n",
    "OUTPUT_DIM = len(answer_vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = answer_vocab['<PAD>'])\n",
    "\n",
    "print(\"Encoder Model:\")\n",
    "print(enc)\n",
    "print(\"\\nDecoder Model:\")\n",
    "print(dec)\n",
    "print(\"\\nSeq2Seq Model:\")\n",
    "print(model)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-13T07:48:39.323445Z",
     "end_time": "2023-05-13T07:48:44.897780Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "import time\n",
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step_losses = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "        src = batch[0].to(device)\n",
    "        trg = batch[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Check if the batch size is equal to BATCH_SIZE\n",
    "        if len(src) == batch_size:\n",
    "            output = model(src, trg)\n",
    "        else:\n",
    "            # Pad the batch to have a length divisible by BATCH_SIZE\n",
    "            padding_length = batch_size - (len(src) % batch_size)\n",
    "            padded_src = torch.cat((src, torch.zeros(padding_length, *src.shape[1:], dtype=src.dtype)), dim=0)\n",
    "            padded_trg = torch.cat((trg, torch.zeros(padding_length, *trg.shape[1:], dtype=trg.dtype)), dim=0)\n",
    "            output = model(padded_src, padded_trg)\n",
    "\n",
    "        output_dim = output.shape[-1]\n",
    "\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        step_losses.append(loss.item())\n",
    "\n",
    "        if (i + 1) % 100 == 0:  # Adjust the frequency based on your preference\n",
    "            elapsed_time = time.time() - start_time\n",
    "            steps_per_sec = (i + 1) / elapsed_time\n",
    "            avg_loss = epoch_loss / (i + 1)\n",
    "            print(f\"Step: {i+1}/{len(iterator)} | Loss: {avg_loss:.4f} | Steps/sec: {steps_per_sec:.2f}\")\n",
    "\n",
    "    return epoch_loss / len(iterator), step_losses\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-13T07:48:52.143657Z",
     "end_time": "2023-05-13T07:48:52.197412Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-13T07:48:53.653823Z",
     "end_time": "2023-05-13T07:48:53.663514Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected hidden[0] size (2, 43, 512), got [2, 40, 512]",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 12\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# Training loop\u001B[39;00m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(N_EPOCHS):\n\u001B[0;32m---> 12\u001B[0m     train_loss, step_loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mCLIP\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     13\u001B[0m     valid_loss \u001B[38;5;241m=\u001B[39m evaluate(model, val_loader, criterion)\n\u001B[1;32m     15\u001B[0m     train_losses\u001B[38;5;241m.\u001B[39mappend(train_loss)\n",
      "Cell \u001B[0;32mIn[11], line 16\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(model, iterator, optimizer, criterion, clip)\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# Check if the batch size is equal to BATCH_SIZE\u001B[39;00m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(src) \u001B[38;5;241m==\u001B[39m batch_size:\n\u001B[0;32m---> 16\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43msrc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     18\u001B[0m     \u001B[38;5;66;03m# Pad the batch to have a length divisible by BATCH_SIZE\u001B[39;00m\n\u001B[1;32m     19\u001B[0m     padding_length \u001B[38;5;241m=\u001B[39m batch_size \u001B[38;5;241m-\u001B[39m (\u001B[38;5;28mlen\u001B[39m(src) \u001B[38;5;241m%\u001B[39m batch_size)\n",
      "File \u001B[0;32m~/.virtualenvs/RNN4/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[0;32mIn[9], line 20\u001B[0m, in \u001B[0;36mSeq2Seq.forward\u001B[0;34m(self, src, trg, teacher_forcing_ratio)\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m trg[\u001B[38;5;241m0\u001B[39m, :]\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, trg_len):\n\u001B[0;32m---> 20\u001B[0m     output, hidden, cell \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecoder\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhidden\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcell\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     22\u001B[0m     outputs[t] \u001B[38;5;241m=\u001B[39m output\n\u001B[1;32m     24\u001B[0m     teacher_force \u001B[38;5;241m=\u001B[39m random\u001B[38;5;241m.\u001B[39mrandom() \u001B[38;5;241m<\u001B[39m teacher_forcing_ratio\n",
      "File \u001B[0;32m~/.virtualenvs/RNN4/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[0;32mIn[8], line 23\u001B[0m, in \u001B[0;36mDecoder.forward\u001B[0;34m(self, input, hidden, cell)\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m     21\u001B[0m embedded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membedding(\u001B[38;5;28minput\u001B[39m))\n\u001B[0;32m---> 23\u001B[0m output, (hidden, cell) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrnn\u001B[49m\u001B[43m(\u001B[49m\u001B[43membedded\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcell\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     25\u001B[0m prediction \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc_out(output\u001B[38;5;241m.\u001B[39msqueeze(\u001B[38;5;241m0\u001B[39m))\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m prediction, hidden, cell\n",
      "File \u001B[0;32m~/.virtualenvs/RNN4/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/.virtualenvs/RNN4/lib/python3.10/site-packages/torch/nn/modules/rnn.py:810\u001B[0m, in \u001B[0;36mLSTM.forward\u001B[0;34m(self, input, hx)\u001B[0m\n\u001B[1;32m    806\u001B[0m     \u001B[38;5;66;03m# Each batch of the hidden state should match the input sequence that\u001B[39;00m\n\u001B[1;32m    807\u001B[0m     \u001B[38;5;66;03m# the user believes he/she is passing in.\u001B[39;00m\n\u001B[1;32m    808\u001B[0m     hx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpermute_hidden(hx, sorted_indices)\n\u001B[0;32m--> 810\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcheck_forward_args\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_sizes\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    811\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m batch_sizes \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    812\u001B[0m     result \u001B[38;5;241m=\u001B[39m _VF\u001B[38;5;241m.\u001B[39mlstm(\u001B[38;5;28minput\u001B[39m, hx, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_flat_weights, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_layers,\n\u001B[1;32m    813\u001B[0m                       \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbidirectional, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_first)\n",
      "File \u001B[0;32m~/.virtualenvs/RNN4/lib/python3.10/site-packages/torch/nn/modules/rnn.py:731\u001B[0m, in \u001B[0;36mLSTM.check_forward_args\u001B[0;34m(self, input, hidden, batch_sizes)\u001B[0m\n\u001B[1;32m    725\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcheck_forward_args\u001B[39m(\u001B[38;5;28mself\u001B[39m,  \u001B[38;5;66;03m# type: ignore[override]\u001B[39;00m\n\u001B[1;32m    726\u001B[0m                        \u001B[38;5;28minput\u001B[39m: Tensor,\n\u001B[1;32m    727\u001B[0m                        hidden: Tuple[Tensor, Tensor],\n\u001B[1;32m    728\u001B[0m                        batch_sizes: Optional[Tensor],\n\u001B[1;32m    729\u001B[0m                        ):\n\u001B[1;32m    730\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcheck_input(\u001B[38;5;28minput\u001B[39m, batch_sizes)\n\u001B[0;32m--> 731\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcheck_hidden_size\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_expected_hidden_size\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_sizes\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    732\u001B[0m \u001B[43m                           \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mExpected hidden[0] size \u001B[39;49m\u001B[38;5;132;43;01m{}\u001B[39;49;00m\u001B[38;5;124;43m, got \u001B[39;49m\u001B[38;5;132;43;01m{}\u001B[39;49;00m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    733\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcheck_hidden_size(hidden[\u001B[38;5;241m1\u001B[39m], \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_expected_cell_size(\u001B[38;5;28minput\u001B[39m, batch_sizes),\n\u001B[1;32m    734\u001B[0m                            \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mExpected hidden[1] size \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m, got \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/.virtualenvs/RNN4/lib/python3.10/site-packages/torch/nn/modules/rnn.py:239\u001B[0m, in \u001B[0;36mRNNBase.check_hidden_size\u001B[0;34m(self, hx, expected_hidden_size, msg)\u001B[0m\n\u001B[1;32m    236\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcheck_hidden_size\u001B[39m(\u001B[38;5;28mself\u001B[39m, hx: Tensor, expected_hidden_size: Tuple[\u001B[38;5;28mint\u001B[39m, \u001B[38;5;28mint\u001B[39m, \u001B[38;5;28mint\u001B[39m],\n\u001B[1;32m    237\u001B[0m                       msg: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mExpected hidden size \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m, got \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    238\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m hx\u001B[38;5;241m.\u001B[39msize() \u001B[38;5;241m!=\u001B[39m expected_hidden_size:\n\u001B[0;32m--> 239\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(msg\u001B[38;5;241m.\u001B[39mformat(expected_hidden_size, \u001B[38;5;28mlist\u001B[39m(hx\u001B[38;5;241m.\u001B[39msize())))\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Expected hidden[0] size (2, 43, 512), got [2, 40, 512]"
     ]
    }
   ],
   "source": [
    "# Set the training parameters\n",
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "# Create lists to store the training progress\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "step_losses = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss, step_loss = train(model, train_loader, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, val_loader, criterion)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    step_losses.extend(step_loss)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Plot the training progress\n",
    "plt.plot(range(1, N_EPOCHS+1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, N_EPOCHS+1), valid_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the step-wise losses\n",
    "plt.plot(range(1, len(step_losses)+1), step_losses)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
