{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T02:58:32.305185Z",
     "start_time": "2023-05-14T02:58:30.152562Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import torch\n",
    "import requests\n",
    "import json\n",
    "from nltk.corpus import brown\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import io\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from collections import Counter\n",
    "import requests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "download and prepare the Brown corpus from NLTK for training a Word2Vec model, which is used for creating embeddings of words."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Download Squad2 dataset, split it into training and testing sets, and save them to files."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T02:59:00.475389Z",
     "start_time": "2023-05-14T02:58:47.434281Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Download SQuAD2 dataset\n",
    "url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\"\n",
    "response = requests.get(url)\n",
    "\n",
    "#Save the dataset to a file\n",
    "dataset_file = \"squad2.json\"\n",
    "with open(dataset_file, 'wb') as f:\n",
    "   f.write(response.content)\n",
    "\n",
    "#Split dataset into train and test files\n",
    "train_ratio = 0.8\n",
    "train_file = \"train.json\"\n",
    "test_file = \"test.json\"\n",
    "\n",
    "with open(dataset_file, 'r') as f:\n",
    "    squad_data = json.load(f)\n",
    "    data = squad_data['data']\n",
    "\n",
    "    # Shuffle the data\n",
    "    random.shuffle(data)\n",
    "\n",
    "    # Split into train and test sets\n",
    "    split_index = int(len(data) * train_ratio)\n",
    "    train_data = data[:split_index]\n",
    "    test_data = data[split_index:]\n",
    "\n",
    "    # Save train data to file\n",
    "    train_squad_data = {'data': train_data}\n",
    "    with open(train_file, 'w') as train_f:\n",
    "        json.dump(train_squad_data, train_f)\n",
    "\n",
    "    # Save test data to file\n",
    "    test_squad_data = {'data': test_data}\n",
    "    with open(test_file, 'w') as test_f:\n",
    "        json.dump(test_squad_data, test_f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T02:59:23.405379Z",
     "start_time": "2023-05-14T02:59:23.382743Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#We will use this function on each file in the dataset - test and train\n",
    "#only the filename is required as the previous function returns the specified file to the same location\n",
    "\n",
    "def extract_squad_data(file_name):\n",
    "    with open(file_name, 'r', encoding='utf-8') as f:\n",
    "        file_name = json.load(f)\n",
    "\n",
    "    paragraphs = []\n",
    "    for article in file_name['data']:\n",
    "        for paragraph in article['paragraphs']:\n",
    "            for qa in paragraph['qas']:\n",
    "                question = qa['question']\n",
    "                answers = [answer['text'] for answer in qa['answers']]\n",
    "                for answer in answers:\n",
    "                    paragraphs.append((question, answer))\n",
    "\n",
    "    df = pd.DataFrame(paragraphs, columns=['Question', 'Answer'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T02:59:25.715029Z",
     "start_time": "2023-05-14T02:59:24.424164Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#convert .json files to dataframes\n",
    "train_df = extract_squad_data(\"train.json\") \n",
    "test_df = extract_squad_data(\"test.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def clean_text(df):\n",
    "    cleaned_df = df.copy()\n",
    "    for column in cleaned_df:\n",
    "        cleaned_df[column] = cleaned_df[column].apply(lambda sentence: process_sentence(sentence))\n",
    "    return cleaned_df\n",
    "\n",
    "def process_sentence(sentence):\n",
    "    sentence = ''.join([s.lower() for s in sentence if s not in string.punctuation])\n",
    "    sentence = ' '.join(stemmer.stem(w) for w in sentence.split())\n",
    "    tokens = nltk.tokenize.RegexpTokenizer(r'\\w+').tokenize(sentence)\n",
    "\n",
    "    return ' '.join(tokens) \n",
    "# usage:\n",
    "# df = clean_text(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean_df = clean_text(train_df.head(50))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_clean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self):\n",
    "        self.word2index = {}\n",
    "        self.index2word = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 4: \"<UNK>\"}\n",
    "        self.word2count = {}\n",
    "        self.n_words = 4  # Count <PAD>, <SOS>, <EOS>, <UNK>\n",
    "    def build_vocab(self, sentences):\n",
    "        for sentence in sentences:\n",
    "            for word in self.tokenize(sentence):\n",
    "                self.add_word(word)\n",
    "    def tokenize(self, sentence):\n",
    "        return sentence.strip().split()\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.word2count[word] = 1\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_vocab = Vocab()\n",
    "answer_vocab = Vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'Vocab' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m question_vocab\u001b[39m.\u001b[39mvocab[\u001b[39m'\u001b[39m\u001b[39m<UNK>\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39;49m(question_vocab)\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'Vocab' has no len()"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_vocab = Vocab()\n",
    "answer_vocab = Vocab()\n",
    "\n",
    "\n",
    "\n",
    "questions = train_clean_df['Question'].tolist()\n",
    "answers = train_clean_df['Answer'].tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'Vocab' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m question_vocab\u001b[39m.\u001b[39mvocab[\u001b[39m'\u001b[39m\u001b[39m<UNK>\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39;49m(question_vocab)\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'Vocab' has no len()"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '<PAD>', 1: '<SOS>', 2: '<EOS>', 4: 'who', 5: 'is', 6: 'the', 7: 'world', 8: 'oldest', 9: 'reign', 10: 'monarch', 11: 'than', 12: 'which', 13: 'queen', 14: 'has', 15: 'elizabeth', 16: 'rule', 17: 'longer', 18: 'how', 19: 'victoria', 20: 'relat', 21: 'to', 22: 'in', 23: 'what', 24: 'year', 25: 'did', 26: 'pass', 27: 'length', 28: 'of', 29: 'histori', 30: 'longest', 31: 'england', 32: 'propos', 33: 'affili', 34: 'with', 35: 'wale', 36: 'quell', 37: 'welsh', 38: 'nation', 39: 'whi', 40: 'britain', 41: 'not', 42: 'want', 43: 'associ', 44: 'conscienti', 45: 'objector', 46: 'join', 47: '1946', 48: 'was', 49: 'alway', 50: 'princ', 51: 'name', 52: 'princess', 53: 'when', 54: 'were', 55: 'and', 56: 'philip', 57: 'marri', 58: 'at', 59: 'famous', 60: 'cathedr', 61: 'mani', 62: 'wed', 63: 'gift', 64: 'receiv', 65: 'design', 66: 'gown', 67: 'former', 68: 'king', 69: 'invit', 70: 'margaret', 71: 'ask', 72: 'do', 73: 'instead', 74: 'mar', 75: 'townsend', 76: 'act', 77: 'would', 78: 'church', 79: 'permit', 80: '1960', 81: 'titl', 82: 'armstrongjon', 83: 'given', 84: 'asid', 85: 'from', 86: 'her', 87: 'choic', 88: 'eden', 89: 'successor', 90: 'crisi', 91: 'caus', 92: 'be', 93: 'critic', 94: 'lord', 95: 'altrincham', 96: 'accus', 97: 'macmillan', 98: 'resign', 99: 'appoint', 100: 'as', 101: 'prime', 102: 'minist', 103: 'after', 104: 'a', 105: 'formal', 106: 'mechan', 107: 'for', 108: 'elect', 109: 'adopt', 110: 'dure', 111: 'australian', 112: 'constitut', 113: 'dismiss', 114: 'whilam', 115: 'post', 116: 'whitlam', 117: 'have', 118: 'hous', 119: 'repres', 120: 'appeal', 121: 'revers', 122: 'declin', 123: 'respons', 124: 'by', 125: 'schole', 126: 'support', 127: 'amend', 128: 'canada', 129: 'politician', 130: 'chang', 131: '1987', 132: 'govern', 133: 'remov', 134: 'coup', 135: 'leader', 136: 'sitiveni', 137: 'rabuka', 138: 'declar', 139: 'fiji', 140: '1991', 141: 'money', 142: 'issu', 143: 'featur', 144: 'public', 145: 'celebr', 146: '2002', 147: 'mother', 148: 'die', 149: 'februari', 150: 'peopl', 151: 'london', 152: 'attend', 153: 'each', 154: 'day', 155: 'three', 156: 'event', 157: 'group', 158: 'surpris', 159: 'approv', 160: 'open', 161: 'summer', 162: 'olymp', 163: 'montreal', 164: 'appear', 165: 'film', 166: 'part', 167: 'ceremoni', 168: 'actor', 169: 'also', 170: 'activ', 171: 'industri', 172: 'bafta', 173: 'award', 174: 'been', 175: 'subject', 176: 'specul', 177: 'concern', 178: 'wealth', 179: 'extim', 180: '1971', 181: 'palac', 182: 'call', 183: 'estim', 184: '1993', 185: 'sunday', 186: 'time', 187: 'fortun', 188: '2015', 189: 'where', 190: 'doe', 191: 'place', 192: 'list', 193: 'richest', 194: 'uk'}\n"
     ]
    }
   ],
   "source": [
    "print(question_vocab.index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_or_cut(df, column_names, length):\n",
    "    pad_symbol = \"<PAD>\"\n",
    "    \n",
    "    for column_name in column_names:\n",
    "        df[column_name] = df[column_name].apply(lambda x: (x.split()[:length] + [pad_symbol]*length)[:length])\n",
    "        df[column_name] = df[column_name].apply(lambda x: ' '.join(x))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'Vocab' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m question_vocab\u001b[39m.\u001b[39mvocab[\u001b[39m'\u001b[39m\u001b[39m<UNK>\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39;49m(question_vocab)\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'Vocab' has no len()"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "padded_df = pad_or_cut(train_clean_df, ['Question', 'Answer'], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'Vocab' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m question_vocab[\u001b[39m'\u001b[39m\u001b[39m<UNK>\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39;49m(question_vocab)\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'Vocab' has no len()"
     ]
    }
   ],
   "source": [
    "question_vocab['<UNK>'] = len(question_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T00:33:27.320885Z",
     "start_time": "2023-05-14T00:33:23.672308Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>who is the world oldest reign monarch &lt;PAD&gt; &lt;P...</td>\n",
       "      <td>elizabeth &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>than which queen has elizabeth rule longer &lt;PA...</td>\n",
       "      <td>queen victoria &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>how is victoria relat to elizabeth &lt;PAD&gt; &lt;PAD&gt;...</td>\n",
       "      <td>greatgreatgrandmoth &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>in what year did elizabeth pass victoria lengt...</td>\n",
       "      <td>2015 &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>in the histori of what is elizabeth the longes...</td>\n",
       "      <td>world histori &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;P...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  \\\n",
       "0  who is the world oldest reign monarch <PAD> <P...   \n",
       "1  than which queen has elizabeth rule longer <PA...   \n",
       "2  how is victoria relat to elizabeth <PAD> <PAD>...   \n",
       "3  in what year did elizabeth pass victoria lengt...   \n",
       "4  in the histori of what is elizabeth the longes...   \n",
       "\n",
       "                                              Answer  \n",
       "0  elizabeth <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> ...  \n",
       "1  queen victoria <PAD> <PAD> <PAD> <PAD> <PAD> <...  \n",
       "2  greatgreatgrandmoth <PAD> <PAD> <PAD> <PAD> <P...  \n",
       "3  2015 <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>...  \n",
       "4  world histori <PAD> <PAD> <PAD> <PAD> <PAD> <P...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'Vocab' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m question_vocab[\u001b[39m'\u001b[39m\u001b[39m<UNK>\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39;49m(question_vocab)\n\u001b[1;32m      2\u001b[0m answer_vocab[\u001b[39m'\u001b[39m\u001b[39m<UNK>\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(answer_vocab)\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'Vocab' has no len()"
     ]
    }
   ],
   "source": [
    "question_vocab['<UNK>'] = len(question_vocab)\n",
    "answer_vocab['<UNK>'] = len(answer_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_to_indices(df, question_vocab, answer_vocab):\n",
    "    df['Question'] = df['Question'].apply(lambda sentence: [question_vocab.word2index[word] if word in question_vocab.word2index else question_vocab.word2index['<UNK>'] for word in sentence.split()])\n",
    "    df['Answer'] = df['Answer'].apply(lambda sentence: [answer_vocab.word2index[word] if word in answer_vocab.word2index else answer_vocab.word2index['<UNK>'] for word in sentence.split()])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'<UNK>'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_655/1453817580.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mindices_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwords_to_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_655/2522957419.py\u001b[0m in \u001b[0;36mwords_to_indices\u001b[0;34m(df, question_vocab, answer_vocab)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mwords_to_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Question'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Question'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mquestion_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquestion_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2index\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mquestion_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<UNK>'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Answer'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Answer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0manswer_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manswer_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2index\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0manswer_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<UNK>'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4355\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4356\u001b[0m         \"\"\"\n\u001b[0;32m-> 4357\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4359\u001b[0m     def _reduce(\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1041\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1043\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 \u001b[0;31m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m                 \u001b[0;31m# \"Callable[[Any], Any]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m                 mapped = lib.map_infer(\n\u001b[0m\u001b[1;32m   1099\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_655/2522957419.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mwords_to_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Question'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Question'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mquestion_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquestion_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2index\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mquestion_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<UNK>'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Answer'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Answer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0manswer_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manswer_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2index\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0manswer_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<UNK>'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_655/2522957419.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mwords_to_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Question'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Question'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mquestion_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquestion_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2index\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mquestion_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<UNK>'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Answer'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Answer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0manswer_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manswer_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2index\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0manswer_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<UNK>'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '<UNK>'"
     ]
    }
   ],
   "source": [
    "indices_df = words_to_indices(padded_df, question_vocab, answer_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[4, 5, 6, 7, 8, 9, 10, 7, 11, 12]</td>\n",
       "      <td>[4, 5, 6, 6, 6, 6, 6, 6, 6, 6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[6, 4, 5, 13, 7, 8, 14, 15, 16, 16]</td>\n",
       "      <td>[4, 5, 6, 6, 6, 6, 6, 6, 6, 6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[4, 5, 10, 17, 18, 7, 19, 20, 21, 7]</td>\n",
       "      <td>[4, 5, 6, 6, 6, 6, 6, 6, 6, 6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[4, 22, 5, 23, 7, 24, 25, 21, 26, 27]</td>\n",
       "      <td>[4, 5, 6, 6, 6, 6, 6, 6, 6, 6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[7, 28, 29, 30, 27, 10, 17, 31, 32, 5]</td>\n",
       "      <td>[4, 5, 7, 6, 6, 6, 6, 6, 6, 6]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Question                          Answer\n",
       "0       [4, 5, 6, 7, 8, 9, 10, 7, 11, 12]  [4, 5, 6, 6, 6, 6, 6, 6, 6, 6]\n",
       "1     [6, 4, 5, 13, 7, 8, 14, 15, 16, 16]  [4, 5, 6, 6, 6, 6, 6, 6, 6, 6]\n",
       "2    [4, 5, 10, 17, 18, 7, 19, 20, 21, 7]  [4, 5, 6, 6, 6, 6, 6, 6, 6, 6]\n",
       "3   [4, 22, 5, 23, 7, 24, 25, 21, 26, 27]  [4, 5, 6, 6, 6, 6, 6, 6, 6, 6]\n",
       "4  [7, 28, 29, 30, 27, 10, 17, 31, 32, 5]  [4, 5, 7, 6, 6, 6, 6, 6, 6, 6]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
