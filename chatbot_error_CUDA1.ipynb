{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install num2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T02:58:32.305185Z",
     "start_time": "2023-05-14T02:58:30.152562Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import torch\n",
    "import requests\n",
    "import json\n",
    "from nltk.corpus import brown\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import io\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "import json\n",
    "from collections import Counter\n",
    "import torch.nn.functional as F\n",
    "import requests\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "import string\n",
    "from nltk.stem import SnowballStemmer\n",
    "import num2words\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import num2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download SQuAD2 dataset\n",
    "url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\"\n",
    "response = requests.get(url)\n",
    "\n",
    "#Save the dataset to a file\n",
    "dataset_file = \"squad2.json\"\n",
    "with open(dataset_file, 'wb') as f:\n",
    "   f.write(response.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T02:59:23.405379Z",
     "start_time": "2023-05-14T02:59:23.382743Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#We will use this function on each file in the dataset - test and train\n",
    "#only the filename is required as the previous function returns the specified file to the same location\n",
    "\n",
    "def extract_squad_data(file_name):\n",
    "    with open(file_name, 'r', encoding='utf-8') as f:\n",
    "        file_name = json.load(f)\n",
    "\n",
    "    paragraphs = []\n",
    "    for article in file_name['data']:\n",
    "        for paragraph in article['paragraphs']:\n",
    "            for qa in paragraph['qas']:\n",
    "                question = qa['question']\n",
    "                answers = [answer['text'] for answer in qa['answers']]\n",
    "                for answer in answers:\n",
    "                    paragraphs.append((question, answer))\n",
    "\n",
    "    df = pd.DataFrame(paragraphs, columns=['Question', 'Answer'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T02:59:25.715029Z",
     "start_time": "2023-05-14T02:59:24.424164Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#convert .json files to dataframes\n",
    "squad_df = extract_squad_data(\"squad2.json\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_word_histogram_side_by_side(dataframe, question_column, answer_column, title):\n",
    "    # Combine all the words from the question_column into a single string\n",
    "    all_question_words = ' '.join(dataframe[question_column].values.tolist())\n",
    "\n",
    "    # Split the string into individual words for the question_column\n",
    "    question_word_list = all_question_words.split()\n",
    "\n",
    "    # Combine all the words from the answer_column into a single string\n",
    "    all_answer_words = ' '.join(dataframe[answer_column].values.tolist())\n",
    "\n",
    "    # Split the string into individual words for the answer_column\n",
    "    answer_word_list = all_answer_words.split()\n",
    "\n",
    "    # Count the frequency of each word for the question_column\n",
    "    question_word_counts = {}\n",
    "    for word in question_word_list:\n",
    "        question_word_counts[word] = question_word_counts.get(word, 0) + 1\n",
    "\n",
    "    # Count the frequency of each word for the answer_column\n",
    "    answer_word_counts = {}\n",
    "    for word in answer_word_list:\n",
    "        answer_word_counts[word] = answer_word_counts.get(word, 0) + 1\n",
    "\n",
    "    # Calculate the frequency buckets for the question_column\n",
    "    question_bucket_counts = [0] * 10\n",
    "    for count in question_word_counts.values():\n",
    "        if count < 5:\n",
    "            question_bucket_counts[0] += 1\n",
    "        elif count < 10:\n",
    "            question_bucket_counts[1] += 1\n",
    "        elif count < 15:\n",
    "            question_bucket_counts[2] += 1\n",
    "        elif count < 20:\n",
    "            question_bucket_counts[3] += 1\n",
    "        elif count < 25:\n",
    "            question_bucket_counts[4] += 1\n",
    "        elif count < 30:\n",
    "            question_bucket_counts[5] += 1\n",
    "        elif count < 35:\n",
    "            question_bucket_counts[6] += 1\n",
    "        elif count < 40:\n",
    "            question_bucket_counts[7] += 1\n",
    "        elif count < 45:\n",
    "            question_bucket_counts[8] += 1\n",
    "        else:\n",
    "            question_bucket_counts[9] += 1\n",
    "\n",
    "    # Calculate the frequency buckets for the answer_column\n",
    "    answer_bucket_counts = [0] * 10\n",
    "    for count in answer_word_counts.values():\n",
    "        if count < 5:\n",
    "            answer_bucket_counts[0] += 1\n",
    "        elif count < 10:\n",
    "            answer_bucket_counts[1] += 1\n",
    "        elif count < 15:\n",
    "            answer_bucket_counts[2] += 1\n",
    "        elif count < 20:\n",
    "            answer_bucket_counts[3] += 1\n",
    "        elif count < 25:\n",
    "            answer_bucket_counts[4] += 1\n",
    "        elif count < 30:\n",
    "            answer_bucket_counts[5] += 1\n",
    "        elif count < 35:\n",
    "            answer_bucket_counts[6] += 1\n",
    "        elif count < 40:\n",
    "            answer_bucket_counts[7] += 1\n",
    "        elif count < 45:\n",
    "            answer_bucket_counts[8] += 1\n",
    "        else:\n",
    "            answer_bucket_counts[9] += 1\n",
    "\n",
    "    # Calculate the total word count (Question + Answer)\n",
    "    total_word_list = question_word_list + answer_word_list\n",
    "    total_word_count = len(total_word_list)\n",
    "\n",
    "    # Set the width of the bars\n",
    "    bar_width = 0.35\n",
    "\n",
    "    # Set the x locations of the bars\n",
    "    r1 = np.arange(len(question_bucket_counts))\n",
    "    r2 = [x + bar_width for x in r1]\n",
    "\n",
    "    # Plot the histogram side by side\n",
    "    plt.bar(r1, question_bucket_counts, color='b', width=bar_width, label=f'{question_column}')\n",
    "    plt.bar(r2, answer_bucket_counts, color='orange', width=bar_width, label=f'{answer_column}')\n",
    "\n",
    "    # Add a separate box/call out for total word count\n",
    "    plt.text(len(question_bucket_counts) / 2, max(max(question_bucket_counts), max(answer_bucket_counts)),\n",
    "             f'Total Words: {total_word_count}', ha='center', va='center', bbox=dict(facecolor='lightgray', edgecolor='black', boxstyle='round'))\n",
    "\n",
    "    # Add labels, title, and legend\n",
    "    plt.xlabel('Frequency Range')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title(title)\n",
    "    plt.xticks([r + bar_width/2 for r in range(len(question_bucket_counts))], ['0-4', '5-9', '10-14', '15-19', '20-24',\n",
    "                                                                        '25-29', '30-34', '35-39', '40-44', '45+'])\n",
    "    plt.legend()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_word_histogram_side_by_side(squad_df, 'Question', 'Answer', 'Word frequency comparision for raw data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This Histogram shows the vast majority of the words appearly very infreqently. \n",
    "# This may negantivitly affect performance as such frequnctly appearing words may have little informational value and act as noise \n",
    "# Hence data the data cleaning below (ext cleaning, tokenising, removing stopwords, removing rare words (<2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample of src/answer pairs for raw datat\n",
    "print(\"Training Vocab trg Raw\")\n",
    "print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "print(squad_df['Question'].head(5))\n",
    "print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "print(\"Testing Vocab src Raw\")\n",
    "print(squad_df['Answer'].head(5))\n",
    "print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#pre-cleaned questions and trg to list\n",
    "raw_src = train_df['Question'].tolist()\n",
    "raw_answers = train_df['Answer'].tolist()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text cleaning, tokenising, removing stopwords, removing rare words (<2)\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.stem import SnowballStemmer\n",
    "from num2words import num2words\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stop_words = set(stopwords.words('english'))  # NLTK's list of English stop words\n",
    "\n",
    "def clean_text(df, min_freq=2):  # min_freq: minimum frequency to keep a token\n",
    "    cleaned_df = df.copy()\n",
    "    all_tokens = []  # List to store all tokens for frequency calculation\n",
    "    \n",
    "    for column in cleaned_df:\n",
    "        cleaned_df[column] = cleaned_df[column].apply(lambda sentence: process_sentence(sentence, all_tokens))\n",
    "        \n",
    "    # Create frequency distribution of tokens\n",
    "    freq_dist = FreqDist(all_tokens)\n",
    "    \n",
    "    # Identify rare tokens\n",
    "    rare_tokens = {token for token, freq in freq_dist.items() if freq < min_freq}\n",
    "    \n",
    "    # Remove rare tokens from the dataframe\n",
    "    for column in cleaned_df:\n",
    "        cleaned_df[column] = cleaned_df[column].apply(lambda sentence: ' '.join(token for token in sentence.split() if token not in rare_tokens))\n",
    "        \n",
    "    return cleaned_df\n",
    "\n",
    "def process_sentence(sentence, all_tokens):\n",
    "    sentence = ''.join([s.lower() for s in sentence if s not in string.punctuation])\n",
    "    sentence = ' '.join(stemmer.stem(w) for w in sentence.split())\n",
    "    \n",
    "    # Convert numbers to words\n",
    "    sentence = ' '.join(convert_num_to_words(w) for w in sentence.split())\n",
    "    \n",
    "    # Remove stop words\n",
    "    sentence = ' '.join(w for w in sentence.split() if w not in stop_words)\n",
    "    \n",
    "    tokens = nltk.tokenize.RegexpTokenizer(r'\\w+').tokenize(sentence)\n",
    "    \n",
    "    # Add tokens to the all_tokens list\n",
    "    all_tokens.extend(tokens)\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def convert_num_to_words(s):\n",
    "    try:\n",
    "        # Remove comma from numbers\n",
    "        s = s.replace(',', '')\n",
    "        # If the string represents a fraction, convert both parts separately\n",
    "        if '/' in s:\n",
    "            numerator, denominator = s.split('/')\n",
    "            return num2words(int(numerator)) + ' over ' + num2words(int(denominator))\n",
    "        # Otherwise, try converting the string to an integer or a float\n",
    "        try:\n",
    "            return num2words(int(s))\n",
    "        except ValueError:\n",
    "            return num2words(float(s))\n",
    "    except:\n",
    "        # If any of the above attempts throw an exception, the string is not a number\n",
    "        return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean raw data files\n",
    "squad_clean_df = clean_text(squad_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histrograme of word fequency for cleaned data\n",
    "plot_word_histogram_side_by_side(squad_clean_df, 'Question', 'Answer', 'Question/Answer Word frequency comparision for Cleaned data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A significant number of words have been removed, especally those in the 0-4 class. \n",
    "# This is to be expected since we removed all words with a frequency of less than 2 and all stop words (that tend to have a frequency of 4 or less)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Vocab Answers Cleaned\")\n",
    "print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "print(squad_clean_df['Question'].head(5))\n",
    "print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "print(\"Testing Vocab src Cleaned\")\n",
    "print(squad_clean_df['Answer'].head(5))\n",
    "print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vocab class that builds vocab, checks for unquie words, and assigns to an index value\n",
    "class Vocab:\n",
    "    def __init__(self):\n",
    "        self.word2index = {}\n",
    "        self.index2word = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
    "        self.word2count = {}\n",
    "        self.n_words = 4  # Count <PAD>, <SOS>, <EOS>, <UNK>\n",
    "\n",
    "    def build_vocab(self, sentences):\n",
    "        unique_words = set()\n",
    "        for sentence in sentences:\n",
    "            for word in self.tokenize(sentence):\n",
    "                unique_words.add(word)\n",
    "\n",
    "        for word in unique_words:\n",
    "            self.add_word(word)\n",
    "\n",
    "    def tokenize(self, sentence):\n",
    "        return sentence.strip().split()\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.word2count[word] = 1\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = squad_clean_df['Question']\n",
    "answers = squad_clean_df['Answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaned question/answer pairs to Vocab\n",
    "question_vocab = Vocab()\n",
    "answer_vocab = Vocab()\n",
    "\n",
    "\n",
    "question_vocab.build_vocab(questions)\n",
    "answer_vocab.build_vocab(answers)\n",
    "\n",
    "question_vocab.add_word('<UNK>')\n",
    "answer_vocab.add_word('<UNK>')\n",
    "\n",
    "# Define and build your vocabularies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(question_vocab.n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counting vocab sizes\n",
    "print(\"Training Set, question Vocab size in unique tokens\")\n",
    "question_vocab_size = len(question_vocab.word2count)\n",
    "print(question_vocab_size)\n",
    "\n",
    "print(\"#############################################\")\n",
    "\n",
    "print(\"Training Set, Answer Vocab size in unique tokens\")\n",
    "answer_vocab_size = len(answer_vocab.word2count)\n",
    "print(answer_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Set, Index to word Question dictionary\")\n",
    "print(question_vocab.index2word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Set, Index to word Answer dictionary\")\n",
    "print(answer_vocab.index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_or_cut(df, column_names, length):\n",
    "    pad_symbol = \"<PAD>\"\n",
    "    \n",
    "    for column_name in column_names:\n",
    "        df[column_name] = df[column_name].apply(lambda x: (x.split()[:length] + [pad_symbol]*length)[:length])\n",
    "        df[column_name] = df[column_name].apply(lambda x: ' '.join(x))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "padded_df = pad_or_cut(squad_clean_df, ['Question', 'Answer'], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_to_indices(df, question_vocab, answer_vocab):\n",
    "    df['Question'] = df['Question'].apply(lambda sentence: [question_vocab.word2index[word] if word in question_vocab.word2index else question_vocab.word2index['<UNK>'] for word in sentence.split()])\n",
    "    df['Answer'] = df['Answer'].apply(lambda sentence: [answer_vocab.word2index[word] if word in answer_vocab.word2index else answer_vocab.word2index['<UNK>'] for word in sentence.split()])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_df = words_to_indices(padded_df, question_vocab, answer_vocab) # could have used nn.padding & nn.embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_df_save = idx_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_df.head(20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class QnADataset(Dataset):\n",
    "    def __init__(self, src, trg):\n",
    "        self.src = src\n",
    "        self.trg = trg\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.src[idx], self.trg[idx]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the proportion of the data you want in your validation and test sets\n",
    "val_size = 0.1\n",
    "test_size = 0.1\n",
    "\n",
    "# Ensure that the validation and test set proportions sum to less than 1\n",
    "assert val_size + test_size < 1\n",
    "\n",
    "# Separate your DataFrame into two separate sets of sequences.\n",
    "src = idx_df['Question'].tolist()\n",
    "trg = idx_df['Answer'].tolist()\n",
    "\n",
    "src_vocab = question_vocab\n",
    "trg_vocab = answer_vocab\n",
    "\n",
    "src_word2index = src_vocab.word2index\n",
    "trg_word2index = trg_vocab.word2index\n",
    "\n",
    "\n",
    "# Convert words in your sequences to their corresponding indices\n",
    "src = [[src_vocab.word2index[word] if word in src_vocab.word2index else src_vocab.word2index['<UNK>'] for word in sequence] for sequence in src]\n",
    "trg = [[trg_vocab.word2index[word] if word in trg_vocab.word2index else trg_vocab.word2index['<UNK>'] for word in sequence] for sequence in trg]\n",
    "\n",
    "# Convert your sequences to PyTorch tensors\n",
    "src = [torch.tensor(item) for item in src]\n",
    "trg = [torch.tensor(item) for item in trg]\n",
    "\n",
    "# First split into train and temp sets\n",
    "src_train, src_temp, trg_train, trg_temp = train_test_split(\n",
    "    src, trg, test_size=(val_size+test_size), random_state=42)\n",
    "\n",
    "# Split the temp set into validation and test sets\n",
    "src_val, src_test, trg_val, trg_test = train_test_split(\n",
    "    src_temp, trg_temp, test_size=test_size/(val_size+test_size), random_state=42)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = QnADataset(src_train, trg_train)\n",
    "val_dataset = QnADataset(src_val, trg_val)\n",
    "test_dataset = QnADataset(src_test, trg_test)\n",
    "\n",
    "# Create DataLoaders for training, validation, and test sets\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxxxxxxxxxxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(src_word2index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, embedding_size, pretrained_embeddings=None):\n",
    "\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        # self.embedding provides a vector representation of the inputs to our model\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "\n",
    "        # Load the pretrained embeddings, if available\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding.weight.data.copy_(torch.from_numpy(pretrained_embeddings))\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size)\n",
    "\n",
    "    def forward(self, i):\n",
    "\n",
    "        embedded = self.embedding(i)\n",
    "\n",
    "        o, (h, c) = self.lstm(embedded)\n",
    "\n",
    "        return o, h, c\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size, output_size, embedding_size, pretrained_embeddings=None):\n",
    "\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        # self.embedding provides a vector representation of the target to our model\n",
    "        self.embedding = nn.Embedding(output_size, embedding_size)\n",
    "\n",
    "        # Load the pretrained embeddings, if available\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding.weight.data.copy_(torch.from_numpy(pretrained_embeddings))\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size)\n",
    "\n",
    "        self.output = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, i, h):\n",
    "\n",
    "        embedded = self.embedding(i)\n",
    "\n",
    "        o, h = self.lstm(embedded, h)\n",
    "\n",
    "        output = self.output(o)\n",
    "\n",
    "        return output, h\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder_input_size, encoder_hidden_size, decoder_hidden_size, decoder_output_size, embedding_size, pretrained_embeddings=None):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = Encoder(encoder_input_size, encoder_hidden_size, embedding_size, pretrained_embeddings)\n",
    "        self.decoder = Decoder(decoder_hidden_size, decoder_output_size, embedding_size, pretrained_embeddings)\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size = trg.shape[1]\n",
    "        max_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output.out_features\n",
    "\n",
    "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(src.device)\n",
    "\n",
    "        # Encode the input sequence\n",
    "        o, h, c = self.encoder(src)\n",
    "\n",
    "        # Initialize the decoder hidden state\n",
    "        decoder_hidden = (h.repeat(self.decoder.lstm.num_layers, 1, 1),\n",
    "                          c.repeat(self.decoder.lstm.num_layers, 1, 1))\n",
    "\n",
    "        # Set the input to the first token of the target sequence\n",
    "        input_token = trg[0, :]\n",
    "\n",
    "        for t in range(1, max_len):\n",
    "            output, decoder_hidden = self.decoder(input_token.unsqueeze(0), decoder_hidden)\n",
    "            outputs[t] = output.squeeze(0)\n",
    "\n",
    "            # Decide whether to use teacher forcing or not\n",
    "            teacher_force = torch.rand(1) < teacher_forcing_ratio\n",
    "\n",
    "            # Set the input token to either the target token or the predicted token\n",
    "            input_token = trg[t] if teacher_force else output.argmax(dim=2).squeeze(0)\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "encoder_input_size = len(src_word2index)\n",
    "encoder_hidden_size = 256\n",
    "decoder_hidden_size = 256\n",
    "decoder_output_size = len(trg_word2index)\n",
    "embedding_size = 100\n",
    "#pretrained_embeddings=None\n",
    "model = Seq2Seq(encoder_input_size, encoder_hidden_size, decoder_hidden_size, decoder_output_size, embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=TRG_vocab['<pad>'])\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training function\n",
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for src, trg in train_loader:\n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(src, trg)\n",
    "\n",
    "        # The output dimensions are [trg len, batch size, output dim] but the loss function expects\n",
    "        # the inputs to be [batch size, output dim] and the targets to be [batch size]\n",
    "        output = output[1:].reshape(-1, output.shape[-1])\n",
    "        trg = trg[1:].reshape(-1)\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "\n",
    "\n",
    "    return total_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src, trg in test_loader:\n",
    "            src, trg = src.to(device), trg.to(device)\n",
    "\n",
    "            output = model(src, trg, teacher_forcing_ratio=0.0)  # Disable teacher forcing during evaluation\n",
    "\n",
    "            # The output dimensions are [trg len, batch size, output dim] but the loss function expects\n",
    "            # the inputs to be [batch size, output dim] and the targets to be [batch size]\n",
    "            output = output[1:].reshape(-1, output.shape[-1])\n",
    "            trg = trg[1:].reshape(-1)\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train with time\n",
    "# Train the model\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "\n",
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()  # Start time for the epoch\n",
    "\n",
    "    train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "    test_loss = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "\n",
    "    end_time = time.time()  # End time for the epoch\n",
    "    epoch_time = end_time - start_time  # Time taken for the epoch\n",
    "\n",
    "    print(f'Epoch: {epoch+1}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Time: {epoch_time:.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Training function\n",
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(iterator):\n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg)\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Hyperparameters\n",
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "# Define an optimizer and a loss function (criterion)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "TRG_PAD_IDX = trg_vocab.word2index['<PAD>']\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss = train(model, train_dataloader, optimizer, criterion, CLIP)\n",
    "    print(f'Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
