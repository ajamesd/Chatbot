{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T02:58:32.305185Z",
     "start_time": "2023-05-14T02:58:30.152562Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import torch\n",
    "import requests\n",
    "import json\n",
    "from nltk.corpus import brown\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import io\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from collections import Counter\n",
    "import requests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "download and prepare the Brown corpus from NLTK for training a Word2Vec model, which is used for creating embeddings of words."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Download Squad2 dataset, split it into training and testing sets, and save them to files."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T02:59:00.475389Z",
     "start_time": "2023-05-14T02:58:47.434281Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Download SQuAD2 dataset\n",
    "url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\"\n",
    "response = requests.get(url)\n",
    "\n",
    "#Save the dataset to a file\n",
    "dataset_file = \"squad2.json\"\n",
    "with open(dataset_file, 'wb') as f:\n",
    "   f.write(response.content)\n",
    "\n",
    "#Split dataset into train and test files\n",
    "train_ratio = 0.8\n",
    "train_file = \"train.json\"\n",
    "test_file = \"test.json\"\n",
    "\n",
    "with open(dataset_file, 'r') as f:\n",
    "    squad_data = json.load(f)\n",
    "    data = squad_data['data']\n",
    "\n",
    "    # Shuffle the data\n",
    "    random.shuffle(data)\n",
    "\n",
    "    # Split into train and test sets\n",
    "    split_index = int(len(data) * train_ratio)\n",
    "    train_data = data[:split_index]\n",
    "    test_data = data[split_index:]\n",
    "\n",
    "    # Save train data to file\n",
    "    train_squad_data = {'data': train_data}\n",
    "    with open(train_file, 'w') as train_f:\n",
    "        json.dump(train_squad_data, train_f)\n",
    "\n",
    "    # Save test data to file\n",
    "    test_squad_data = {'data': test_data}\n",
    "    with open(test_file, 'w') as test_f:\n",
    "        json.dump(test_squad_data, test_f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T02:59:23.405379Z",
     "start_time": "2023-05-14T02:59:23.382743Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#We will use this function on each file in the dataset - test and train\n",
    "#only the filename is required as the previous function returns the specified file to the same location\n",
    "\n",
    "def extract_squad_data(file_name):\n",
    "    with open(file_name, 'r', encoding='utf-8') as f:\n",
    "        file_name = json.load(f)\n",
    "\n",
    "    paragraphs = []\n",
    "    for article in file_name['data']:\n",
    "        for paragraph in article['paragraphs']:\n",
    "            for qa in paragraph['qas']:\n",
    "                question = qa['question']\n",
    "                answers = [answer['text'] for answer in qa['answers']]\n",
    "                for answer in answers:\n",
    "                    paragraphs.append((question, answer))\n",
    "\n",
    "    df = pd.DataFrame(paragraphs, columns=['Question', 'Answer'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T02:59:25.715029Z",
     "start_time": "2023-05-14T02:59:24.424164Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#convert .json files to dataframes\n",
    "train_df = extract_squad_data(\"train.json\") \n",
    "test_df = extract_squad_data(\"test.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def clean_text(df):\n",
    "    cleaned_df = df.copy()\n",
    "    for column in cleaned_df:\n",
    "        cleaned_df[column] = cleaned_df[column].apply(lambda sentence: process_sentence(sentence))\n",
    "    return cleaned_df\n",
    "\n",
    "def process_sentence(sentence):\n",
    "    sentence = ''.join([s.lower() for s in sentence if s not in string.punctuation])\n",
    "    sentence = ' '.join(stemmer.stem(w) for w in sentence.split())\n",
    "    tokens = nltk.tokenize.RegexpTokenizer(r'\\w+').tokenize(sentence)\n",
    "\n",
    "    return ' '.join(tokens) \n",
    "# usage:\n",
    "# df = clean_text(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean_df = clean_text(train_df.head(50))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Question         Answer\n",
      "0  what citi in the unit state has the highest popul       new york\n",
      "1               in what citi is the unit nation base       new york\n",
      "2  what citi has been call the cultur capit of th...       new york\n",
      "3  what american citi welcom the largest number o...       new york\n",
      "4  the major gateway for immigr has been which us...  new york citi\n",
      "5  the most popul citi in the unit state is which...  new york citi\n",
      "6             how mani borough compris new york citi           five\n",
      "7  in what year were the five borough combin into...           1898\n",
      "8  in 2014 what did the census estim the popul of...        8491079\n",
      "9    what is the size of new york citi in squar mile            305\n"
     ]
    }
   ],
   "source": [
    "print(train_clean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self):\n",
    "        self.word2index = {}\n",
    "        self.index2word = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 4: \"<UNK>\"}\n",
    "        self.word2count = {}\n",
    "        self.n_words = 4  # Count <PAD>, <SOS>, <EOS>, <UNK>\n",
    "    def build_vocab(self, sentences):\n",
    "        for sentence in sentences:\n",
    "            for word in self.tokenize(sentence):\n",
    "                self.add_word(word)\n",
    "    def tokenize(self, sentence):\n",
    "        return sentence.strip().split()\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.word2count[word] = 1\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_vocab = Vocab()\n",
    "answer_vocab = Vocab()\n",
    "\n",
    "questions = train_clean_df['Question'].tolist()\n",
    "answers = train_clean_df['Answer'].tolist()\n",
    "\n",
    "question_vocab.build_vocab(questions)\n",
    "answer_vocab.build_vocab(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '<PAD>', 1: '<SOS>', 2: '<EOS>', 4: 'what', 5: 'is', 6: 'a', 7: 'sourc', 8: 'for', 9: 'the', 10: 'particl', 11: 'in', 12: 'paper', 13: 'common', 14: 'use', 15: 'last', 16: 'step', 17: 'product', 18: 'when', 19: 'was', 20: 'pulp', 21: 'papermak', 22: 'process', 23: 'develop', 24: 'countri', 25: 'lead', 26: 'of', 27: 'second', 28: 'largest', 29: 'produc', 30: 'which', 31: 'court', 32: 'eunuch', 33: 'associ', 34: 'with', 35: 'who', 36: 'credit', 37: 'materi', 38: 'did', 39: 'replac', 40: 'upon', 41: 'it', 42: 'invent', 43: 'age', 44: 'contribut', 45: 'toward', 46: 'centuri', 47: 'first', 48: 'attribut', 49: 'to', 50: 'spread', 51: 'from', 52: 'china', 53: 'europ', 54: 'middl', 55: 'eastern', 56: 'citi', 57: 'name', 58: 'take', 59: 'on', 60: 'baghdad', 61: 'becom', 62: 'much', 63: 'cheaper', 64: 'than', 65: 'befor', 66: 'canadian', 67: 'inventor', 68: 'help', 69: 'fg', 70: 'keller', 71: 'their', 72: 'own', 73: 'latin', 74: 'word', 75: 'deriv', 76: 'languag', 77: 'papyrus', 78: 'chang', 79: 'properti', 80: 'fibr', 81: 'plant', 82: 'egypt', 83: 'make', 84: 'part', 85: 'cyperus', 86: 'separ', 87: 'cellulos', 88: 'type', 89: 'made', 90: 'chemic', 91: 'known', 92: 'as', 93: 'percentag', 94: 'are', 95: 'wast', 96: 'bleach', 97: 'cotton', 98: 'how', 99: 'mani', 100: 'there', 101: 'decad', 102: 'doe', 103: 'sulfit', 104: 'date', 105: 'most', 106: 'anoth', 107: 'straw', 108: 'high', 109: 'silic', 110: 'content', 111: 'besid', 112: 'thermomechan', 113: 'main', 114: 'ingredi', 115: 'groundwood', 116: 'strength', 117: 'by', 118: 'mechan', 119: 'recylc', 120: 'can', 121: 'be', 122: 'new', 123: 'clay', 124: 'improv', 125: 'characterist', 126: 'ad', 127: 'size', 128: 'purpos', 129: 'filler', 130: 'water', 131: 'remov', 132: 'forc', 133: 'sheet', 134: 'collect', 135: 'expel', 136: 'press', 137: 'handmak', 138: 'blotter', 139: 'compon', 140: 'dri', 141: 'at', 142: 'moistur', 143: 'tri', 144: 'achiev', 145: 'end', 146: 'calcium', 147: 'coat', 148: 'has', 149: 'thin', 150: 'layer', 151: 'done', 152: 'polish', 153: 'surfac', 154: 'shiniest', 155: 'give', 156: 'best', 157: 'optic', 158: 'densiti', 159: 'contrapt', 160: 'carri', 161: 'web', 162: 'print', 163: 'manner', 164: 'normal', 165: 'cut', 166: 'if', 167: 'not'}\n"
     ]
    }
   ],
   "source": [
    "print(question_vocab.index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_or_cut(df, column_names, length):\n",
    "    pad_symbol = \"<PAD>\"\n",
    "    \n",
    "    for column_name in column_names:\n",
    "        df[column_name] = df[column_name].apply(lambda x: (x.split()[:length] + [pad_symbol]*length)[:length])\n",
    "        df[column_name] = df[column_name].apply(lambda x: ' '.join(x))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_df = pad_or_cut(train_clean_df, ['Question', 'Answer'], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T00:33:27.320885Z",
     "start_time": "2023-05-14T00:33:23.672308Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what is a sourc for the particl in paper &lt;PAD&gt;</td>\n",
       "      <td>wood &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what is a common use for paper &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt;</td>\n",
       "      <td>write &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what is the last step in paper product &lt;PAD&gt; &lt;...</td>\n",
       "      <td>dri &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>when was the pulp papermak process develop &lt;PA...</td>\n",
       "      <td>2nd centuri ad &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what countri was the process develop in &lt;PAD&gt; ...</td>\n",
       "      <td>china &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  \\\n",
       "0     what is a sourc for the particl in paper <PAD>   \n",
       "1   what is a common use for paper <PAD> <PAD> <PAD>   \n",
       "2  what is the last step in paper product <PAD> <...   \n",
       "3  when was the pulp papermak process develop <PA...   \n",
       "4  what countri was the process develop in <PAD> ...   \n",
       "\n",
       "                                              Answer  \n",
       "0  wood <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>...  \n",
       "1  write <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD...  \n",
       "2  dri <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> ...  \n",
       "3  2nd centuri ad <PAD> <PAD> <PAD> <PAD> <PAD> <...  \n",
       "4  china <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_to_indices(df, question_vocab, answer_vocab):\n",
    "    df['Question'] = df['Question'].apply(lambda sentence: [question_vocab.word2index[word] if word in question_vocab.word2index else question_vocab.word2index['<UNK>'] for word in sentence.split()])\n",
    "    df['Answer'] = df['Answer'].apply(lambda sentence: [answer_vocab.word2index[word] if word in answer_vocab.word2index else answer_vocab.word2index['<UNK>'] for word in sentence.split()])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'<UNK>'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_655/1453817580.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mindices_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwords_to_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_655/2522957419.py\u001b[0m in \u001b[0;36mwords_to_indices\u001b[0;34m(df, question_vocab, answer_vocab)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mwords_to_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Question'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Question'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mquestion_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquestion_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2index\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mquestion_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<UNK>'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Answer'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Answer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0manswer_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manswer_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2index\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0manswer_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<UNK>'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4355\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4356\u001b[0m         \"\"\"\n\u001b[0;32m-> 4357\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4359\u001b[0m     def _reduce(\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1041\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1043\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 \u001b[0;31m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m                 \u001b[0;31m# \"Callable[[Any], Any]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m                 mapped = lib.map_infer(\n\u001b[0m\u001b[1;32m   1099\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_655/2522957419.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mwords_to_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Question'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Question'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mquestion_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquestion_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2index\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mquestion_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<UNK>'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Answer'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Answer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0manswer_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manswer_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2index\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0manswer_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<UNK>'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_655/2522957419.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mwords_to_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Question'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Question'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mquestion_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquestion_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2index\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mquestion_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<UNK>'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Answer'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Answer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0manswer_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manswer_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2index\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0manswer_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<UNK>'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '<UNK>'"
     ]
    }
   ],
   "source": [
    "indices_df = words_to_indices(padded_df, question_vocab, answer_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[4, 5, 6, 7, 8, 9, 10, 7, 11, 12]</td>\n",
       "      <td>[4, 5, 6, 6, 6, 6, 6, 6, 6, 6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[6, 4, 5, 13, 7, 8, 14, 15, 16, 16]</td>\n",
       "      <td>[4, 5, 6, 6, 6, 6, 6, 6, 6, 6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[4, 5, 10, 17, 18, 7, 19, 20, 21, 7]</td>\n",
       "      <td>[4, 5, 6, 6, 6, 6, 6, 6, 6, 6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[4, 22, 5, 23, 7, 24, 25, 21, 26, 27]</td>\n",
       "      <td>[4, 5, 6, 6, 6, 6, 6, 6, 6, 6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[7, 28, 29, 30, 27, 10, 17, 31, 32, 5]</td>\n",
       "      <td>[4, 5, 7, 6, 6, 6, 6, 6, 6, 6]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Question                          Answer\n",
       "0       [4, 5, 6, 7, 8, 9, 10, 7, 11, 12]  [4, 5, 6, 6, 6, 6, 6, 6, 6, 6]\n",
       "1     [6, 4, 5, 13, 7, 8, 14, 15, 16, 16]  [4, 5, 6, 6, 6, 6, 6, 6, 6, 6]\n",
       "2    [4, 5, 10, 17, 18, 7, 19, 20, 21, 7]  [4, 5, 6, 6, 6, 6, 6, 6, 6, 6]\n",
       "3   [4, 22, 5, 23, 7, 24, 25, 21, 26, 27]  [4, 5, 6, 6, 6, 6, 6, 6, 6, 6]\n",
       "4  [7, 28, 29, 30, 27, 10, 17, 31, 32, 5]  [4, 5, 7, 6, 6, 6, 6, 6, 6, 6]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
